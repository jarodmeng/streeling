geom_function(fun = dt_scaled, args = list(df = n-1, mean = mu, sd = s1_se)) +
geom_function(fun = dt_scaled, args = list(df = n-1, mean = s1_mean, sd = s1_se), color = "red") +
geom_vline(xintercept = mu) +
geom_vline(xintercept = s1_mean, color = "red") +
geom_vline(xintercept = mu - z*s1_se, linetype = "dashed") +
geom_vline(xintercept = mu + z*s1_se, linetype = "dashed")
mu <- 100
n <- 50
sample_mean <- 98
sample_sd <- 15
alpha = 0.05
set.seed(4400)
s1 <- rnorm(n = n, mean = sample_mean, sd = sample_sd)
s1_mean = mean(s1)
s1_sd = sd(s1)
s1_se = s1_sd / sqrt(n)
hist(s1)
t <- (s1_mean - mu) / s1_se
p.value <- 2 * pt(abs(t), df = n-1, lower.tail = FALSE)
z <- qt(alpha/2, df = n-1, lower.tail = FALSE)
s1_lower <- s1_mean - z * s1_se
s1_upper <- s1_mean + z * s1_se
t.test(s1, alternative = "two.sided", mu = mu, conf.level = 0.95)
power.t.test(delta = abs(s1_mean - mu), sd = s1_sd, sig.level = alpha, power = 0.8, type = "one.sample", alternative = "two.sided")
power.t.test(n = n, delta = abs(s1_mean - mu), sd = s1_sd, sig.level = alpha, type = "one.sample", alternative = "two.sided")
pwr.t.test(d = abs(s1_mean - mu)/s1_sd, sig.level = alpha, power = 0.8, type = "one.sample", alternative = "two.sided")
pwr.t.test(n = n, d = abs(s1_mean - mu)/s1_sd, sig.level = alpha, type = "one.sample", alternative = "two.sided")
dt_scaled <- function(x, df, mean, sd) {
dt((x - mean)/sd, df)
}
ggplot() +
xlim(s1_mean - 5*s1_se, mu + 5*s1_se) +
geom_function(fun = dt_scaled, args = list(df = n-1, mean = mu, sd = s1_se)) +
geom_function(fun = dt_scaled, args = list(df = n-1, mean = s1_mean, sd = s1_se), color = "red") +
geom_vline(xintercept = mu) +
geom_vline(xintercept = s1_mean, color = "red") +
geom_vline(xintercept = mu - z*s1_se, linetype = "dashed") +
geom_vline(xintercept = mu + z*s1_se, linetype = "dashed")
beta = pt((mu - z*s1_se - s1_mean)/s1_se, df = n-1, lower.tail = FALSE)
power = 1-beta
power
p1 = pt((mu - z*s1_se - s1_mean)/s1_se, df = n-1, lower.tail = FALSE)
p2 = pt((mu + z*s1_se - s1_mean)/s1_se, df = n-1, lower.tail = FALSE)
p1
p2
beta = p1-p2
power = 1-beta
power
beta = p1+p2
power = 1-beta
power
p1
p2
1-p1
pwr.t.test(n = n, d = abs(s1_mean - mu)/s1_sd, sig.level = alpha, type = "one.sample", alternative = "two.sided")
beta
power
beta = p1+p2
beta
p1
beta = p1-p2
power = 1-beta
power
p1
1-p1
p1 = pt((mu - z*s1_se - s1_mean)/s1_se, df = n-1, lower.tail = FALSE)
p2 = pt((mu + z*s1_se - s1_mean)/s1_se, df = n-1, lower.tail = FALSE)
beta = p1-p2
power = 1-beta
power
pwr.t.test(n = n, d = abs(s1_mean - mu)/s1_sd, sig.level = alpha, type = "one.sample", alternative = "two.sided")
mu <- 100
n <- 1000
sample_mean <- 98
sample_sd <- 15
alpha = 0.05
set.seed(4400)
s1 <- rnorm(n = n, mean = sample_mean, sd = sample_sd)
s1_mean = mean(s1)
s1_sd = sd(s1)
s1_se = s1_sd / sqrt(n)
hist(s1)
t <- (s1_mean - mu) / s1_se
p.value <- 2 * pt(abs(t), df = n-1, lower.tail = FALSE)
z <- qt(alpha/2, df = n-1, lower.tail = FALSE)
s1_lower <- s1_mean - z * s1_se
s1_upper <- s1_mean + z * s1_se
t.test(s1, alternative = "two.sided", mu = mu, conf.level = 0.95)
power.t.test(delta = abs(s1_mean - mu), sd = s1_sd, sig.level = alpha, power = 0.8, type = "one.sample", alternative = "two.sided")
power.t.test(n = n, delta = abs(s1_mean - mu), sd = s1_sd, sig.level = alpha, type = "one.sample", alternative = "two.sided")
pwr.t.test(d = abs(s1_mean - mu)/s1_sd, sig.level = alpha, power = 0.8, type = "one.sample", alternative = "two.sided")
pwr.t.test(n = n, d = abs(s1_mean - mu)/s1_sd, sig.level = alpha, type = "one.sample", alternative = "two.sided")
dt_scaled <- function(x, df, mean, sd) {
dt((x - mean)/sd, df)
}
ggplot() +
xlim(s1_mean - 5*s1_se, mu + 5*s1_se) +
geom_function(fun = dt_scaled, args = list(df = n-1, mean = mu, sd = s1_se)) +
geom_function(fun = dt_scaled, args = list(df = n-1, mean = s1_mean, sd = s1_se), color = "red") +
geom_vline(xintercept = mu) +
geom_vline(xintercept = s1_mean, color = "red") +
geom_vline(xintercept = mu - z*s1_se, linetype = "dashed") +
geom_vline(xintercept = mu + z*s1_se, linetype = "dashed")
p1 = pt((mu - z*s1_se - s1_mean)/s1_se, df = n-1, lower.tail = FALSE)
p2 = pt((mu + z*s1_se - s1_mean)/s1_se, df = n-1, lower.tail = FALSE)
beta = p1-p2
power = 1-beta
power
p1
p2
library(pak)
pkg_install("tidymodels")
pkg_install("mdsr")
library(tidyverse)
library(RcppRoll)
df <- read_csv(
"https://data.sccgov.org/api/views/6cnm-gchg/rows.csv?accessType=DOWNLOAD"
)
df %>%
mutate(
new_cases_7d = roll_mean(
New_cases, 7, na.rm = TRUE, align = "right", fill = NA
)
) %>%
ggplot(aes(x = Date, y = new_cases_7d)) +
geom_line() +
scale_x_date(date_breaks = "1 month", date_labels = "%m-%d")
library(tidyverse)
library(mdsr)
url <-
"http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
census <- read_csv(
url,
col_names = c(
"age", "workclass", "fnlwgt", "education",
"education_1", "marital_status", "occupation", "relationship",
"race", "sex", "capital_gain", "capital_loss", "hours_per_week",
"native_country", "income"
)
) %>%
mutate(income = factor(income))
census
glimpse(census)
?glimpse
skim(census)
class(census)
?skim
skimr::skim(census)
library(tidymodels)
set.seed(364)
n <- nrow(census)
n
?initial_split
census_parts <- census %>%
initial_split(prop = 0.8)
class(census_parts)
train <- census_parts %>%
training()
test <- census_parts %>%
testing()
list(train, test) %>%
map_int(nrow)
(
pi_bar <- train %>%
count(income) %>%
mutate(pct = n / sum(n)) %>%
filter(income == ">50K") %>%
pull(pct)
)
pi_bar
train
skimr::skim(train)
levels(train$income)
train %>%
count(income) %>%
mutate(pct = n / sum(n))
vocab("generalized linear models")
?vocab
func("left_join")
?logistic_reg
vocab("parsnip")
?logistic_reg
mod_null <- logistic_reg(mode = "classification") %>%
set_engine("glm") %>%
fit(income ~ 1, data = train)
mod_null
library(yardstick)
?predict
train %>%
select(income, capital_gain)
train %>%
select(income, capital_gain) %>%
bind_cols(
predict(mod_null, new_data = train, type = "class")
)
library(yardstick)
pred <- train %>%
select(income, capital_gain) %>%
bind_cols(
predict(mod_null, new_data = train, type = "class")
) %>%
rename(income_null = .pred_class)
accuracy(pred, income, income_null)
mod_null
confusion_null <- pred %>%
conf_mat(truth = income, estimate = income_null)
confusion_null
mod_log_1 <- logistic_reg(mode = "classification") %>%
set_engine("glm") %>%
fit(income ~ capital_gain, data = train)
mod_log_1
train_plus <- train %>%
mutate(high_earner = as.integer(income == ">50K"))
train_plus
ggplot(train_plus, aes(x = capital_gain, y = high_earner)) +
geom_count(
position = position_jitter(width = 0, height = 0.05),
alpha = 0.5
) +
geom_smooth(
method = "glm", method.args = list(family = "binomial"),
color = "dodgerblue", lty = 2, se = FALSE
) +
geom_hline(aes(yintercept = 0.5), linetype = 3) +
scale_x_log10(labels = scales::dollar)
pred <- pred %>%
bind_cols(
predict(mod_log_1, new_data = train, type = "class")
) %>%
rename(income_log_1 = .pred_class)
confusion_log_1 <- pred %>%
conf_mat(truth = income, estimate = income_log_1)
confusion_log_1
accuracy(pred, income, income_log_1)
?autoplot
autoplot(confusion_null) +
geom_label(
aes(
x = (xmax + xmin) / 2,
y = (ymax + ymin) / 2,
label = c("TN", "FP", "FN", "TP")
)
)
autoplot(confusion_log_1) +
geom_label(
aes(
x = (xmax + xmin) / 2,
y = (ymax + ymin) / 2,
label = c("TN", "FP", "FN", "TP")
)
)
broom::tidy(mod_log_1)
mod_log_all <- logistic_reg(mode = "classification") %>%
set_engine("glm") %>%
fit(
income ~ age + workclass + education + marital_status +
occupation + relationship + race + sex +
capital_gain + capital_loss + hours_per_week,
data = train
)
pred <- pred %>%
bind_cols(
predict(mod_log_all, new_data = train, type = "class")
) %>%
rename(income_log_all = .pred_class)
pred %>%
conf_mat(truth = income, estimate = income_log_all)
pred %>%
conf_mat(truth = income, estimate = income_log_all)
confusion_log_all <- pred %>%
conf_mat(truth = income, estimate = income_log_all)
confusion_log_all
accuracy(pred, income, income_log_all)
?confusion_log_all
?initial_split
library(rsample)
set.seed(8584)
bt_resamples <- bootstraps(mtcars, times = 3)
bt_resamples
class(bt_resamples)
class(bt_resamples$splits[[1]])
bt_resamples$splits[[1]]
first_resample <- bt_resamples$splits[[1]]
class(first_resample)
first_resample
analysis(first_resample)
assessment(first_resample)
analysis(first_resample) %>% class
mod_null
pred
pred <- train %>%
select(income, capital_gain) %>%
bind_cols(
predict(mod_null, new_data = train, type = "class")
) %>%
rename(income_null = .pred_class)
pred
predict(mod_null, new_data = train, type = "class")
class(mod_null)
class(mod_null)
predict.glm
debugonce(predict)
predict(mod_null, new_data = train, type = "class")
?predict
?parsnip::predict.model_fit
?accuracy
yardstick::accuracy(data = pred, truth = income, estimate = income_null)
yardstick::accuracy_vec(truth = income, estimate = income_null)
yardstick::accuracy_vec(truth = pred$income, estimate = pred$income_null)
yardstick::accuracy(data = pred, truth = income, estimate = income_null)
?conf_mat
(
confusion_null <- pred %>%
yardstick::conf_mat(truth = income, estimate = income_null)
)
class(confusion_null)
summary(confusion_null)
n1 = 77741
mean1 = 3.21
sd1 = 0.014
n2 = 90049
mean2=3.93
sd2 = 0.003
t = (mean2-mean1)/sqrt(sd1^2/n1+sd2^2/n2)
t
df = n1+n2-2
df
pt(t, df, lower.tail = FALSE)
t = (mean2-mean1)/sqrt(sd1^2/n1+sd2^2/n2)
t
n1 = 77741
mean1 = 3.21
se1 = 0.014
n2 = 90049
mean2=3.93
se2 = 0.003
t = (mean2-mean1)/sqrt(se1^2+se2^2)
t
library(pak)
pkg_install("primes")
library(primes)
?primes
primes
tibble(x = primes)
library(tidyverse)
x <- tibble(x = primes)
x
x %>% mutate(y = seq_along(primes) * log(seq_along(primes)))
x %<>% mutate(y = seq_along(primes) * log(seq_along(primes)))
library(magrittr)
x %<>% mutate(y = seq_along(primes) * log(seq_along(primes)))
x
ggplot(x) + geom_line()
ggplot(x, aes(x,y)) + geom_line()
library(tidyverse)
library(googlesheets4)
library(lubridate)
library(rvest)
library(magrittr)
main_folder <- "/Volumes/Element/jav/JAV_output"
folders <- list.files(main_folder)
get_videos <- function(f) {
tibble(
actress = f,
video = tools::file_path_sans_ext(
list.files(file.path(main_folder, f), pattern = "nfo")
)
)
}
my_movies <- map_dfr(folders, get_videos)
my_movies %<>%
mutate(has_subtitle = str_detect(video, "-[cC]")) %>%
mutate(video = str_replace_all(video, "-[cC]", ""))
my_movies <- separate_rows(my_movies, actress, sep = ",")
nrow(my_movies)
my_movies %>% count(actress) %>% arrange(desc(n))
my_movies %>% count(actress) %>% arrange(desc(n)) %>% print(n=100)
get_page_movies <- function(id, page = 1) {
main_page <- paste0("https://javdb.com/actors/", id)
param_sdc <- "t=s,d,c"
if (page == 1) {
url <- paste0(main_page, "?", param_sdc)
} else {
url <- paste0(main_page, "?page=", page, "&", param_sdc)
}
res <- read_html(url)
has_next <- res %>%
html_nodes(".pagination-list") %>%
html_nodes("a[rel=next]") %>%
length() %>%
{. > 0}
actor_name <- res %>% html_nodes(".actor-section-name") %>% html_text()
movies <- res %>%
html_nodes(".column.grid-item")
dat <- tibble(
actress = actor_name,
id = html_text(html_nodes(movies, ".uid")),
date = ymd(str_trim(html_text(html_nodes(movies, ".meta")))),
title = html_text(html_nodes(movies, ".video-title"))
)
if (has_next) {
dat <- bind_rows(
dat,
get_page_movies(id, page + 1)
)
}
dat %>% separate_rows(actress, sep = ", ")
}
m <- get_page_movies("qPer")
actor <- "林ゆな"
x <- my_movies %>% filter(actress == actor)
y <- m %>% filter(actress == actor)
left_join(y, x, by = c("id" = "video")) %>%
select(-title) %>%
arrange(desc(date)) %>%
print(n = 100)
my_movies %>% count(actress) %>% arrange(desc(n)) %>% print(n=100)
m <- get_page_movies("N4YG")
actor <- "天海つばさ"
x <- my_movies %>% filter(actress == actor)
y <- m %>% filter(actress == actor)
left_join(y, x, by = c("id" = "video")) %>%
select(-title) %>%
arrange(desc(date)) %>%
print(n = 100)
setwd("~/github/tarnortalon/streeling")
blogdown:::serve_site()
Pr <- function(k) {
choose(k-1, 99)/choose(k+99, k)
}
curve(Pr, from = 100, to = 200, xlab = "k", ylab = "Pr(100,k)")
Pr <- function(k) {
choose(k-1, 99)/choose(k+99, k)
}
library(ggplot2)
ggplot() +
xlim(100, 200) +
geom_function(fun = Pr) +
labs(x = "k", y = "Pr(100, k)")
theme_set(theme_minimal())
ggplot() +
xlim(100, 200) +
geom_function(fun = Pr) +
labs(x = "k", y = "Pr(100, k)")
ggplot() +
xlim(100, 30000) +
geom_function(fun = Pr) +
labs(x = "k", y = "Pr(100, k)")
Sim <- function(k, n) {
S <- seq(1, n, 1)
sims <- replicate(1000, sample(S, k, replace = TRUE), simplify = FALSE)
sum(sapply(sims, function(x) length(setdiff(S, x)) == 0))/1000
}
Sim_100 <- function(k) {
sapply(k, function(k) Sim(100, k))
}
Pr <- function(n, k) {
out <- vector("numeric", n+1)
for (j in seq(0, n, 1)) {
out[j+1] <- (-1)^j * ((n-j)/n)^k * choose(n, j)
}
max(0, sum(out))
}
Pr_100 <- function(k) {
sapply(k, function(k) Pr(100, k))
}
ggplot() +
xlim(100, 200) +
geom_function(fun = Pr_100) +
labs(x = "k", y = "Pr(100, k)")
ggplot() +
xlim(100, 1000) +
geom_function(fun = Pr_100) +
labs(x = "k", y = "Pr(100, k)")
for (k in seq(100, 1000, 1)) {
if (Pr_100(k) >= 0.5) {
print(paste0("It takes ", k, " times for the probability to reach 50%."))
break
}
}
for (k in seq(100, 1000, 1)) {
if (Pr_100(k) >= 0.8) {
print(paste0("It takes ", k, " times for the probability to reach 80%."))
break
}
}
Sim <- function(k, n) {
S <- seq(1, n, 1)
sims <- replicate(1000, sample(S, k, replace = TRUE), simplify = FALSE)
sum(sapply(sims, function(x) length(setdiff(S, x)) == 0))/1000
}
Sim_100 <- function(k) {
sapply(k, function(k) Sim(100, k))
}
ggplot() +
xlim(100, 1000) +
geom_function(fun = Pr_100) +
geom_function(fun = Sim_100, args = list(n = 200), color = "red") +
labs(x = "k", y = "Pr(100, k)")
Sim <- function(k, n) {
S <- seq(1, n, 1)
sims <- replicate(1000, sample(S, k, replace = TRUE), simplify = FALSE)
sum(sapply(sims, function(x) length(setdiff(S, x)) == 0))/1000
}
Sim_100 <- function(k, n) {
sapply(k, function(k) Sim(k, n))
}
ggplot() +
xlim(100, 1000) +
geom_function(fun = Pr_100) +
geom_function(fun = Sim_100, args = list(n = 100), color = "red") +
labs(x = "k", y = "Pr(100, k)")
